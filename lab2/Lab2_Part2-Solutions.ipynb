{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 2: Computer Vision\n",
    "\n",
    "# Part 2: CNN for Pneumothorax Detection \n",
    "\n",
    "This second section of the lab will introduce you to using a convolutional network to tackle a realistic dataset in medical diagnostics. Specifically, we use the [ChestXRay dataset](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community), a set of X-ray images labeled with corresponding diagnoses.\n",
    "\n",
    "## 2.1 ChestXRay Dataset\n",
    "\n",
    "The ChestXRay dataset consists of over 100,000 different X-rays from more than 30,000 patients labeled with different disease conditions. Our goal for this lab is to use a subset of the dataset to train a classifier that is able to accurately infer the presence of pneumothorax. \n",
    "\n",
    "Pneumothorax is a condition that occurs when there are abnormal amounts of air in the space between the lung and chest wall. This reduces the capacity to which the lung is able to expand and fill with air, leading to oxygen shortage and low blood pressure. Lack of treatment may lead to worsening symptoms and even death.\n",
    "\n",
    "We provide two images: the image on the left does not have pneumothorax and the image on the right does have pneumothorax.\n",
    "\n",
    "![alt text](img/pneumothorax.png \"Pneumothorax Images\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading the dataset. We provide a sample of the dataset in h5py format, where we extracted positive and negative examples of pneumothorax and downscaled images to $256 \\times 256$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.download_lung_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a class to hold the dataset. This class will allow us to access the training and test data separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PneumothoraxDataset:\n",
    "    def __init__(self):\n",
    "        print(\"Loading X-Ray Dataset!\")\n",
    "\n",
    "        train = h5py.File(util.download_lung_data.data_dir+'pneumothorax_train.h5','r')\n",
    "        test = h5py.File(util.download_lung_data.data_dir+'pneumothorax_test.h5','r')\n",
    "\n",
    "        self.X_train = train['image'][:]\n",
    "        self.X_test = test['image'][:]\n",
    "        self.Y_train = train['label'][:]\n",
    "        self.Y_test = test['label'][:]\n",
    "\n",
    "        self.num_train = self.X_train.shape[0]\n",
    "        self.num_test = self.X_test.shape[0]\n",
    "\n",
    "        self.batch_pointer = 0\n",
    "        \n",
    "    def getTotalNumDataPoints(self):\n",
    "        return self.num_train+self.num_test\n",
    "    \n",
    "    def getTrainBatch(self, batch_size):\n",
    "        inds = np.arange(self.batch_pointer,self.batch_pointer+batch_size)\n",
    "        inds = np.mod( inds , self.num_train ) #cycle through dataset\n",
    "        batch = (self.X_train[inds], self.Y_train[inds]) #grab batch\n",
    "\n",
    "        self.batch_pointer += batch_size #increment counter before returning\n",
    "        return batch\n",
    "\n",
    "    def getTestBatch(self, batch_size):\n",
    "        inds = np.random.choice(self.num_test, size=batch_size)\n",
    "        return (self.X_test[inds], self.Y_test[inds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate this class so we can make calls to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''TODO: Instantiate the clas for this dataset.'''\n",
    "data = PneumothoraxDataset() # TODO\n",
    "print(\"Dataset consists of {} images\".format(data.getTotalNumDataPoints()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Noisy Data!\n",
    "This dataset contains a huge variety of patients and is extremely noisy (i.e., images taken from various angles, scales, illuminations). To demonstrate the difficulty of this dataset, we've randomly picked 4 training images of patients with (right) and without (left) a pneumothorax below.\n",
    "\n",
    "![alt text](img/samples.png \"Pneumothorax Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change value of INDEX here to visualize an image in the dataset!\n",
    "INDEX = 12\n",
    "\n",
    "image = data.X_train[INDEX]\n",
    "label = data.Y_train[INDEX]\n",
    "pred = np.argmax(label)\n",
    "\n",
    "plt.imshow(image[:,:,0], cmap='gray')\n",
    "print(\"This X-Ray \"+(\"HAS\" if pred else \"DOES NOT have\")+ \" a pneumothorax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural Network Model for Pneumothorax Detection\n",
    "First let's start by importing the CNN model that we'll use to detect a pneumothorax.  We've provided this data in `models.py` - you are encouraged to experiment with changes inside this model to improve the accuracy and generalizability (however this is not necessary for this lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.models import PneumothoraxDetectionModel\n",
    "model = PneumothoraxDetectionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Lab 1 we will need to define a set of hyperparameters to control training and testing our algorithm. Feel free to change them to see what works best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "learning_rate = 0.05\n",
    "num_training_steps = int(1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost defines the empirical loss of the model given a set of labels and predictions. \n",
    "'''TODO: Fill in the cost function and arguments by looking at the model. Remember to keep track of logits!'''\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=model.y, logits=model.y_)) # todo remove the labels and logits\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define metrics to measure accuracy (percent correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model.y_, 1), tf.argmax(model.y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "'''TODO: Fill in the model prediction.'''\n",
    "prediction = tf.argmax(model.y_,1) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorboard** is a tool to automatically plot and visualize the training process. You can periodically send \"summaries\" such as: \n",
    "* scalars (loss, accuracy, ...)\n",
    "* images (photos, heatmaps, ...)\n",
    "* distributions (weights, biases, activations, ...)\n",
    "\n",
    "Let's set some simple scalar summaries to be plotted on Tensorboard (http://localhost:6006) - the plotting code below occurs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('cost',cost) \n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all() #combine into a single summary which we can run on Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Training (READ BEFORE RUNNING)\n",
    "**Since this is a very large model, training for this large dataset takes a long time without GPU compute power. So, we have provided you with a model that was obtained by running the following cell. You don't have to run this cell!!!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "uniq_id = \"./logs/lab2part2_\"+uuid.uuid1().__str__()[:6]\n",
    "summary_writer_train = tf.summary.FileWriter(uniq_id+'train', graph=tf.get_default_graph())\n",
    "summary_writer_test = tf.summary.FileWriter(uniq_id+'test', graph=tf.get_default_graph())\n",
    "\n",
    "for step in range(num_training_steps):\n",
    "    (x_batch, y_batch) = data.getTrainBatch(batch_size) # get a training batch of data\n",
    "    _, trainLoss, summary = sess.run([optimizer, cost, merged_summary_op],\n",
    "                               feed_dict={model.x: x_batch, model.y:y_batch})\n",
    "\n",
    "    summary_writer_train.add_summary(summary, step) \n",
    "\n",
    "    if step % 10 == 0:\n",
    "        (x_test, y_test) = data.getTestBatch(100) # get a testing batch of data\n",
    "        testLoss, testAcc, summary = sess.run([cost, accuracy, merged_summary_op], \n",
    "                                              feed_dict={model.x: x_test, model.y:y_test})\n",
    "\n",
    "        print(\"step: {}, train: {}, \\t\\t test: {}, testAcc: {}\".format(\n",
    "              step, trainLoss, testLoss, int(testAcc*100)))\n",
    "        summary_writer_test.add_summary(summary, step)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      save_path = saver.save(sess, uniq_id+'/model.ckpt')\n",
    "      print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Load Trained Model\n",
    "We've provided you with a network which we've trained using the code above. Run the line below to import it and continue with the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#restore the weights from a trained model\n",
    "saver.restore(sess, \"/notebooks/introtodeeplearning_labs/lab2/saved_model/model.ckpt\") #only run this once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Evaluating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wish to evaluate our network on disease prediction of pneumothorax. In diagonostics, it is awkward to evaluate a network based off just pure prediction accuracy. Suppose for example, that our network were to predict all people to have disease. This network would then acheive 100% accuracy in predicting the disease but would be effectively useless.\n",
    "\n",
    "Instead, to evaulate our network, we use the ROC score. In a ROC curve, we plot the positive predictive value (true positives / (true positives + false positives)) against the negative predictive value (true negatives / (true negatives + false negatives)). We can get different positive and negative prediction values by choosing a different confidence threshold at which we consider a disease to be postiive or negative. The area under the curve is then used as a metric to evaluate the quality of a model. Note that under this classification scheme, a model which assigns all outputs to be either positive or negative would have an area under the curve of effectively zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the documentation of these functions online to find out their parameters and their outputs\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(y_true, y_score):\n",
    "    \"\"\" Computes Receiving Operating Characteristic curve and area\n",
    "    \n",
    "    Params:\n",
    "        - y_true: Ground truth binary labels\n",
    "        - y_score: Continuous predictions in the range [0,1]\n",
    "        \n",
    "    Returns:\n",
    "        - fpr: False Positive Rate at different thresholds\n",
    "        - tpr: True Positive Rate at different thresholds\n",
    "        - area: Area under the Receiving Operating Characteristic curve\n",
    "    \"\"\"\n",
    "    '''TODO: Use the functions suggested above to fill in the ROC characteristics'''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score) # TODO\n",
    "    auroc = auc(fpr, tpr) # TODO\n",
    "    return fpr, tpr, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score, title='Receiver operating characteristic example'):\n",
    "    \"\"\" Plots Receiving Operating Characteristic curve\n",
    "    \n",
    "    Params:\n",
    "        - y_true: Ground truth binary labels\n",
    "        - y_score: Continuous predictions in the range [0,1]\n",
    "    \"\"\"\n",
    "    fpr, tpr, auroc = compute_roc(y_true, y_score)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.grid()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = {:.2f})'.format(auroc))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computing the ROC metric we need the true labels of the examples along with the predictions of the network. True labels will be discrete (0:No Pneumothorax, 1:Pneumothorax). Predictions need to be continuous values in the range $[0,1]$ so we can sweep the threshold from 0 to 1 and get the True Postive Rate and False Positive Rate at each point. Since the output layer is a softmax of the two classes, we can interpret the continuous outputs as probabilities and use those directly for the ROC computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test set predictions\n",
    "NUMBER_TEST_SAMPLES = 10\n",
    "\n",
    "y_true = []\n",
    "y_score = []\n",
    "for i in tqdm(range(NUMBER_TEST_SAMPLES)): #compute one at a time due to memory constraints\n",
    "    y_true.extend( data.Y_test[[i],0] )\n",
    "    probs = sess.run(model.probabilities, feed_dict={model.x: [data.X_test[i]]})\n",
    "    y_score.extend( probs[:,0] )\n",
    "    \n",
    "correct = np.array(y_true) == np.round(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy = %2.2f%%\" % (np.mean(correct)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the results as an ROC curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(y_true, y_score, 'Receiver operating characteristic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpreting the Neural Network's Output\n",
    "\n",
    "Although CNNs are an effective tool for learning complex functional mappings, the large number of parameters that bestow such learning capacity often preclude interpretability. This lack of intepretability makes it difficult to analyze and debug complex neural models especially with our highly non-convex optimization landscape. Fortunately, the past few years have seen numerous advances in both theoretical and empirical analyses of CNNs, and in this section, we will explore some examples of the latter.\n",
    "\n",
    "### 2.4.1 [Class Activation Mapping (CAM)](https://arxiv.org/pdf/1512.04150.pdf)\n",
    "\n",
    "Class activation mapping (a technique developed [here at MIT](http://cnnlocalization.csail.mit.edu/)) generates \"heatmaps\" indicating the regions of the image to which our model \"attends\" in the final layers. Note that CAM visualization pertains to architectures with a global average pooling layer preceding the final FC/softmax layer. This is a common architectural choice for classification and localization tasks since it drastically reduces the parameter count of our network.\n",
    "\n",
    "Given a single input image, let $f_k(x,y)$ represent the $k^{th}$ slice/channel of the feature map produced by the last convolution layer. Let $g_k = \\sum_{x,y} {f_k(x,y)}$ denote the output of our global average pooling layer and $s_c = \\sum_k {w_{c,k} g_k}$ denote the unnormalized class scores (logits) produced by the FC layer. Then, the class activation map for a given class $c'$ is given by\n",
    "\n",
    "$$\n",
    "m_c(x,y) = \\sum_k {w_{c,k} \\cdot f_k(x,y)}\n",
    "$$\n",
    "\n",
    "i.e., a dot product between the activations spanning the channel dimension of our final feature map with the FC weights for class $c$.\n",
    "\n",
    "The weights $w_{c,k}$ denote the \"importance\" of the $k^{th}$ feature map in determining the class score. As a result, pixels with large activations in \"important\" layers are assigned greater weight in our class activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>   Can the neural network explain its decision?</h1></center>\n",
    "![alt text](img/heatmap_example.png \"Example Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Preliminaries\n",
    "\n",
    "In order to compute the CAM, we first need to extract the appropriate feature map from an input image. Examine how the `extract_features_weights` function below retrieves the final feature map (i.e., our $f_k$ values) and FC weights from our PneumothoraxDetectionModel model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_weights(sess, model):\n",
    "    \"\"\" Extracts final feature maps and FC weights from trained model.\n",
    "    \n",
    "    Params:\n",
    "        - sess: the current Tensorflow Session where the model was loaded\n",
    "        - model: the PneumothoraxDetectionModel\n",
    "    \n",
    "    Returns (tuple):\n",
    "        - an (_ x 16 x 16 x 512) tf.Tensor of activations from the final convolutional layer\n",
    "        - an (512 x 2) tf.Tensor with the learned weights of the final FC layer\n",
    "    \"\"\"\n",
    "    #access feature map activations directly from the model declaration\n",
    "    feature_maps = model.skip4 \n",
    "    \n",
    "    #access the weights by searching by name\n",
    "    dense_weights = sess.graph.get_tensor_by_name( os.path.split(model.y_.name)[0] + '/kernel:0')\n",
    "    \n",
    "    return (feature_maps, dense_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: extract the features and weights using the function defined directly above '''\n",
    "(feature_maps, dense_weights) = extract_features_weights(sess, model) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Maps: \"+str(feature_maps))\n",
    "print(\"Dense Weights: \"+str(dense_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 CAM Computation\n",
    "\n",
    "Next, let's compute the CAM for a few sample images. In order to visualize the relevant image regions, we'll need to upsample our CAM to our image size. Complete the `compute_cam` and `upsample` functions below.\n",
    "\n",
    "Hint: the `tf.image` module contains common image processing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cam(class_index, fmap, weights):\n",
    "    \"\"\" Forms a CAM operation given a class name, feature maps, and weights\n",
    "    \n",
    "    Params: \n",
    "        - class_index: index of the class to measure\n",
    "        - fmap: (1 x 16 x 16 x 512) tf.Tensor of activations from the final convolutional layer\n",
    "        - weights: (512 x 2) tf.Tensor with the learned weights of the final FC layer\n",
    "    \n",
    "    Returns: \n",
    "        - (16 x 16) tf.Tensor of downscaled CAMs  \n",
    "    \"\"\"\n",
    "    w_vec = tf.expand_dims(weights[:, class_index], 1) \n",
    "    \n",
    "    _, h, w, c = fmap.shape.as_list()\n",
    "    fmap = tf.squeeze(fmap) # remove batch dim\n",
    "    fmap = tf.reshape(fmap, [h * w, c])\n",
    "    \n",
    "    '''TODO: compute the CAM! Remeber to look at the equation defining CAMs above to do this '''\n",
    "    CAM = tf.matmul(fmap, w_vec) # TODO\n",
    "    CAM = tf.reshape(CAM, [1, h, w, 1]) \n",
    "    \n",
    "    return CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO: compute the CAM for a pneumothorax detection (recall what class index this is) using the function above'''\n",
    "cam = compute_cam(1, feature_maps, dense_weights) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(cam, im_hw):\n",
    "    \"\"\" Upsamples CAM to appropriate size\n",
    "    \n",
    "    Params:\n",
    "        - cam: a x_x_ tf.Tensor\n",
    "        - im_hw: target size in [H, W] format\n",
    "        \n",
    "    Returns:\n",
    "        - Upsampled CAM with size _xHxW\n",
    "    \"\"\"\n",
    "    '''TODO: upsampling function call. Hint: look at resize functions in tf.image'''\n",
    "    upsampled = tf.image.resize_bilinear(cam, im_hw) #TODO\n",
    "    return upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TODO: upsample the CAM Tensor to a 256\\times 256 image using the function above '''\n",
    "cam_upsampled = upsample(cam, [256,256]) #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 CAM Heatmaps!\n",
    "Finally, lets put everything together and compute and overlay the CAM heatmap on top of the X-Ray scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_cam(image, cam, save_file=None):\n",
    "    \"\"\" Visualize class activation heatmap, overlaying on image.\n",
    "    \n",
    "    Params:\n",
    "        - image: ndarray of size\n",
    "    \"\"\"\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min()) # TODO: check\n",
    "\n",
    "    plt.imshow(255-image.squeeze(), cmap=plt.cm.gray, vmin=0, vmax=255) \n",
    "    plt.imshow(1-cam, cmap=plt.cm.jet, alpha=0.5, interpolation='nearest', vmin=0, vmax=1)\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample x-rays, predictions, and CAMs overlayed ontop\n",
    "inds = [79,37,45,29,30]\n",
    "for im, cl in zip(data.X_test[inds], data.Y_test[inds]):\n",
    "\n",
    "    heatmap = sess.run(\n",
    "        cam_upsampled,\n",
    "        feed_dict={\n",
    "            model.x: im[np.newaxis, :, :],\n",
    "        })\n",
    "\n",
    "    vis_cam(im, np.squeeze(heatmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Things to Ponder: \n",
    "- Try to find examples of images where the model fails. Are these reasonable failures (difficult images)?\n",
    "- When the model fails, what is it \"looking\" at? \n",
    "- Does the model ever \"look\" at points in the image that don't make sense? What are possible explanations of this?\n",
    "- How can we make simple changes to the model to overcome some of these errors?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
